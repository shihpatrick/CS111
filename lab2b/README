NAME: Patrick Shih

SortedList.c: I discussed with a peer in the class about the best way to implement the list, and we thought that a circular, doubly linked list would be best. This is due to the fact that the head is NULL, so there would be more implementation needed if we added an “end” to the list as also NULL. It also makes use of the resources we have to put the head as the “end.”

lab2_add.c: followed the instructions of the spec on implementation

lab2_list.c: followed the instructions of the spec on implementation.


2.1.1
It takes a greater amount of iterations (and an increase of threads) for the possibilities of collisions/race conditions. The larger the amount of iterations/threads, the more collisions; therefore, more errors. 

A significantly smaller number of iterations so seldom fail because of the same reason stated above. There are less iterations/threads, so when executing through the critical section, the probability of race condition/collision is less.

2.1.2
--yield runs slower because yield interrupts threads. That means they are in waiting state and wait for a critical section to be free. After this, a context switch occur and it can run the critical section. The extra time is the process explained above (interrupt, state changing, waiting for critical section to open, loading queue, running critical section).

No, it is not possible to get a valid timing, because it is indeterministic on how long the yield process takes. Because it is indeterministic, there is no consistent way of determining a valid time.

2.1.3
The average cost per operation drop with increase iterations because with more iterations, the more threads being created. Because of the overhead is fixed, the greater the amount of iterations will grow linearly while the growth of overhang is fixed. As there is a greater number of iterations, the average cost will drop.

Determining the number of iteration to run is based on the local minimum of the graph. If we’re trying to find the optimal cost, we would have to look at where the dip is in the parabola to see its relation.

2.1.4
All the options run similarly for low number of threads because there isn’t much overhead time as there are less threads. Because there are less threads interacting with each other, the locks do not have much affect on the actual overhead the exists within the code.

The three operations slow down as the number of threads rises because there’s an increase in collisions/race conditions with a greater number of threads. Each types of operations would deal with collisions in different ways, it would create some overhead and stall times.

2.2.1
Both of these graphs are both rather linear; however, for add function, it is not linear. This can be seen in the issue with my lab that the add-function was not completely included in the graphs. However, if its expected to be linear, this would make sense, as the cost of operation would increase with more threads, as there is more threads competing and waiting for resources. The cost per iteration in add is much greater than that in list, as the y-axis is scaled to 1000.


2.2.2
Both of these graphs are expected to be linear, however the spin lock levels out at 8 threads. I am unsure of the reasoning behind why the spin locks does not follow a linear relationship. But what can expected is that: similar to mutexes, the linear relationship for both is due to more threads needing more time to compete and access critical sections. Mutex has a slightly higher cost, and more evident in threads 8-30. But from what we can see in threads 1-4, it is not a significant difference.
